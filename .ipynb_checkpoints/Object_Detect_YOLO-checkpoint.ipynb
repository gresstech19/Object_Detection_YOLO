{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "gc2sB0zZnBgI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from seaborn import color_palette\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jF-HKAhJvJ6G"
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lpaOS3zCvq6p",
    "outputId": "8c2b94ba-daeb-4296-bfac-723b9ff0cda5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "o3XntwNbu-28",
    "outputId": "7e264d51-5169-4808-83fe-85e2502a8f49"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c963b6244cf03abad2defc60e7bb0b5dde7cfda",
    "colab": {},
    "colab_type": "code",
    "id": "td0X-DXDnBgg"
   },
   "outputs": [],
   "source": [
    "_BATCH_NORM_DECAY = 0.9\n",
    "_BATCH_NORM_EPSILON = 1e-05\n",
    "_LEAKY_RELU = 0.1\n",
    "_ANCHORS = [(10, 13), (16, 30), (33, 23),\n",
    "            (30, 61), (62, 45), (59, 119),\n",
    "            (116, 90), (156, 198), (373, 326)]\n",
    "_MODEL_SIZE = (416, 416)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e88ca967006cf686c53568071030cb968359716",
    "colab_type": "text",
    "id": "arWOwl0znBg3"
   },
   "source": [
    "`_MODEL_SIZE` refers to the input size of the model.\n",
    "\n",
    "Let's look at other parameters step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80c2ae4a51e7bc745f50997234ed9b738315b19b",
    "colab": {},
    "colab_type": "code",
    "id": "iOIMae6JnBg6"
   },
   "outputs": [],
   "source": [
    "def batch_norm(inputs, training, data_format):\n",
    "    \"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n",
    "    return tf.layers.batch_normalization(\n",
    "        inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n",
    "        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON,\n",
    "        scale=True, training=training)\n",
    "\n",
    "\n",
    "def fixed_padding(inputs, kernel_size, data_format):\n",
    "    \"\"\"ResNet implementation of fixed padding.\n",
    "\n",
    "    Pads the input along the spatial dimensions independently of input size.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor input to be padded.\n",
    "        kernel_size: The kernel to be used in the conv2d or max_pool2d.\n",
    "        data_format: The input format.\n",
    "    Returns:\n",
    "        A tensor with the same format as the input.\n",
    "    \"\"\"\n",
    "    pad_total = kernel_size - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n",
    "                                        [pad_beg, pad_end],\n",
    "                                        [pad_beg, pad_end]])\n",
    "    else:\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n",
    "                                        [pad_beg, pad_end], [0, 0]])\n",
    "    return padded_inputs\n",
    "\n",
    "\n",
    "def conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1):\n",
    "    \"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n",
    "    if strides > 1:\n",
    "        inputs = fixed_padding(inputs, kernel_size, data_format)\n",
    "\n",
    "    return tf.layers.conv2d(\n",
    "        inputs=inputs, filters=filters, kernel_size=kernel_size,\n",
    "        strides=strides, padding=('SAME' if strides == 1 else 'VALID'),\n",
    "        use_bias=False, data_format=data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5361e186fecd4ab9c53ee37987d905f0917f18d4",
    "colab_type": "text",
    "id": "j-p4bCwXnBhL"
   },
   "source": [
    "### Feature extraction: Darknet-53\n",
    "For feature extraction Yolo uses Darknet-53 neural net pretrained on ImageNet. Same as ResNet,  Darknet-53 has shortcut (residual) connections, which help information from earlier layers flow further. We omit the last 3 layers (Avgpool, Connected and Softmax) since we only need the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1fdc7ef8b92f0fdd60ff57911edf48cafe49d83",
    "colab": {},
    "colab_type": "code",
    "id": "JDt1FpHQnBhO"
   },
   "outputs": [],
   "source": [
    "def darknet53_residual_block(inputs, filters, training, data_format,\n",
    "                             strides=1):\n",
    "    \"\"\"Creates a residual block for Darknet.\"\"\"\n",
    "    shortcut = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters=filters, kernel_size=1, strides=strides,\n",
    "        data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters=2 * filters, kernel_size=3, strides=strides,\n",
    "        data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs += shortcut\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def darknet53(inputs, training, data_format):\n",
    "    \"\"\"Creates Darknet53 model for feature extraction.\"\"\"\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = darknet53_residual_block(inputs, filters=32, training=training,\n",
    "                                      data_format=data_format)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(2):\n",
    "        inputs = darknet53_residual_block(inputs, filters=64,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(8):\n",
    "        inputs = darknet53_residual_block(inputs, filters=128,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    route1 = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(8):\n",
    "        inputs = darknet53_residual_block(inputs, filters=256,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    route2 = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(4):\n",
    "        inputs = darknet53_residual_block(inputs, filters=512,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    return route1, route2, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1bcefa6e17216948560c6c2551197abb87b0c131",
    "colab_type": "text",
    "id": "QJZrM9KnnBhc"
   },
   "source": [
    "### Convolution layers\n",
    "Yolo has a large number of convolutional layers. It's useful to group them in blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42c6af6b2a1cc052b69cbb768bfdfeed60e627a0",
    "colab": {},
    "colab_type": "code",
    "id": "afNxeuGSnBhe"
   },
   "outputs": [],
   "source": [
    "def yolo_convolution_block(inputs, filters, training, data_format):\n",
    "    \"\"\"Creates convolution operations layer used after Darknet.\"\"\"\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    route = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    return route, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6f6e900de2d5cd3a94d362d639fdde58f1a1707",
    "colab_type": "text",
    "id": "0uaSvp1LnBhr"
   },
   "source": [
    "### Detection layers\n",
    "Yolo has 3 detection layers, that detect on 3 different scales using respective anchors. For each cell in the feature map the detection layer predicts `n_anchors * (5 + n_classes)` values using 1x1 convolution. For each scale we have `n_anchors = 3`. `5 + n_classes` means that respectively to each of 3 anchors we are going to predict 4 coordinates of the box, its confidence score (the probability of containing an object) and class probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9aa8b0d0f0e297787fe809353107ae379d1c7d24",
    "colab": {},
    "colab_type": "code",
    "id": "Y2sazY5VnBht"
   },
   "outputs": [],
   "source": [
    "def yolo_layer(inputs, n_classes, anchors, img_size, data_format):\n",
    "    \"\"\"Creates Yolo final detection layer.\n",
    "\n",
    "    Detects boxes with respect to anchors.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor input.\n",
    "        n_classes: Number of labels.\n",
    "        anchors: A list of anchor sizes.\n",
    "        img_size: The input size of the model.\n",
    "        data_format: The input format.\n",
    "\n",
    "    Returns:\n",
    "        Tensor output.\n",
    "    \"\"\"\n",
    "    n_anchors = len(anchors)\n",
    "\n",
    "    inputs = tf.layers.conv2d(inputs, filters=n_anchors * (5 + n_classes),\n",
    "                              kernel_size=1, strides=1, use_bias=True,\n",
    "                              data_format=data_format)\n",
    "\n",
    "    shape = inputs.get_shape().as_list()\n",
    "    grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3]\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
    "    inputs = tf.reshape(inputs, [-1, n_anchors * grid_shape[0] * grid_shape[1],\n",
    "                                 5 + n_classes])\n",
    "\n",
    "    strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1])\n",
    "\n",
    "    box_centers, box_shapes, confidence, classes = \\\n",
    "        tf.split(inputs, [2, 2, 1, n_classes], axis=-1)\n",
    "\n",
    "    x = tf.range(grid_shape[0], dtype=tf.float32)\n",
    "    y = tf.range(grid_shape[1], dtype=tf.float32)\n",
    "    x_offset, y_offset = tf.meshgrid(x, y)\n",
    "    x_offset = tf.reshape(x_offset, (-1, 1))\n",
    "    y_offset = tf.reshape(y_offset, (-1, 1))\n",
    "    x_y_offset = tf.concat([x_offset, y_offset], axis=-1)\n",
    "    x_y_offset = tf.tile(x_y_offset, [1, n_anchors])\n",
    "    x_y_offset = tf.reshape(x_y_offset, [1, -1, 2])\n",
    "    box_centers = tf.nn.sigmoid(box_centers)\n",
    "    box_centers = (box_centers + x_y_offset) * strides\n",
    "\n",
    "    anchors = tf.tile(anchors, [grid_shape[0] * grid_shape[1], 1])\n",
    "    box_shapes = tf.exp(box_shapes) * tf.to_float(anchors)\n",
    "\n",
    "    confidence = tf.nn.sigmoid(confidence)\n",
    "\n",
    "    classes = tf.nn.sigmoid(classes)\n",
    "\n",
    "    inputs = tf.concat([box_centers, box_shapes,\n",
    "                        confidence, classes], axis=-1)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa025a0ba6b8656f12d94402d932f471b7e21e80",
    "colab_type": "text",
    "id": "8Fw1C0BZnBh8"
   },
   "source": [
    "### Upsample layer\n",
    "In order to concatenate with shortcut outputs from Darknet-53 before applying detection on a different scale, we are going to upsample the feature map using nearest neighbor interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31a240155d9f96edf06011a4b51d4cd0a999d24d",
    "colab": {},
    "colab_type": "code",
    "id": "0Aqb2Tm2nBh-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def upsample(inputs, out_shape, data_format):\n",
    "    \"\"\"Upsamples to `out_shape` using nearest neighbor interpolation.\"\"\"\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
    "        new_height = out_shape[3]\n",
    "        new_width = out_shape[2]\n",
    "    else:\n",
    "        new_height = out_shape[2]\n",
    "        new_width = out_shape[1]\n",
    "\n",
    "    inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width))\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49717e7dcca05da2c9131c67f303c7554ce7a708",
    "colab_type": "text",
    "id": "FGB0w6Y0nBiG"
   },
   "source": [
    "### Non-max suppression\n",
    "The model is going to produce a lot of boxes, so we need a way to discard the boxes with low confidence scores. Also, to avoid having multiple boxes for one object, we will discard the boxes with high overlap as well using non-max suppresion for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a32f6e7a74477dc1fe77b99ddcf72a776fea3a92",
    "colab": {},
    "colab_type": "code",
    "id": "glpIse6-nBiJ"
   },
   "outputs": [],
   "source": [
    "def build_boxes(inputs):\n",
    "    \"\"\"Computes top left and bottom right points of the boxes.\"\"\"\n",
    "    center_x, center_y, width, height, confidence, classes = \\\n",
    "        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n",
    "\n",
    "    top_left_x = center_x - width / 2\n",
    "    top_left_y = center_y - height / 2\n",
    "    bottom_right_x = center_x + width / 2\n",
    "    bottom_right_y = center_y + height / 2\n",
    "\n",
    "    boxes = tf.concat([top_left_x, top_left_y,\n",
    "                       bottom_right_x, bottom_right_y,\n",
    "                       confidence, classes], axis=-1)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def non_max_suppression(inputs, n_classes, max_output_size, iou_threshold,\n",
    "                        confidence_threshold):\n",
    "    \"\"\"Performs non-max suppression separately for each class.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor input.\n",
    "        n_classes: Number of classes.\n",
    "        max_output_size: Max number of boxes to be selected for each class.\n",
    "        iou_threshold: Threshold for the IOU.\n",
    "        confidence_threshold: Threshold for the confidence score.\n",
    "    Returns:\n",
    "        A list containing class-to-boxes dictionaries\n",
    "            for each sample in the batch.\n",
    "    \"\"\"\n",
    "    batch = tf.unstack(inputs)\n",
    "    boxes_dicts = []\n",
    "    for boxes in batch:\n",
    "        boxes = tf.boolean_mask(boxes, boxes[:, 4] > confidence_threshold)\n",
    "        classes = tf.argmax(boxes[:, 5:], axis=-1)\n",
    "        classes = tf.expand_dims(tf.to_float(classes), axis=-1)\n",
    "        boxes = tf.concat([boxes[:, :5], classes], axis=-1)\n",
    "\n",
    "        boxes_dict = dict()\n",
    "        for cls in range(n_classes):\n",
    "            mask = tf.equal(boxes[:, 5], cls)\n",
    "            mask_shape = mask.get_shape()\n",
    "            if mask_shape.ndims != 0:\n",
    "                class_boxes = tf.boolean_mask(boxes, mask)\n",
    "                boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes,\n",
    "                                                              [4, 1, -1],\n",
    "                                                              axis=-1)\n",
    "                boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1])\n",
    "                indices = tf.image.non_max_suppression(boxes_coords,\n",
    "                                                       boxes_conf_scores,\n",
    "                                                       max_output_size,\n",
    "                                                       iou_threshold)\n",
    "                class_boxes = tf.gather(class_boxes, indices)\n",
    "                boxes_dict[cls] = class_boxes[:, :5]\n",
    "\n",
    "        boxes_dicts.append(boxes_dict)\n",
    "\n",
    "    return boxes_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "46fde971771dab00f4061af6b0641159988fb608",
    "colab_type": "text",
    "id": "UGBVncA-nBiW"
   },
   "source": [
    "### Final model class\n",
    "Finally, let's define the model class using all of the layers described previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "rBO8HpRMnBiY"
   },
   "outputs": [],
   "source": [
    "class Yolo_v3:\n",
    "    \"\"\"Yolo v3 model class.\"\"\"\n",
    "\n",
    "    def __init__(self, n_classes, model_size, max_output_size, iou_threshold,\n",
    "                 confidence_threshold, data_format=None):\n",
    "        \"\"\"Creates the model.\n",
    "\n",
    "        Args:\n",
    "            n_classes: Number of class labels.\n",
    "            model_size: The input size of the model.\n",
    "            max_output_size: Max number of boxes to be selected for each class.\n",
    "            iou_threshold: Threshold for the IOU.\n",
    "            confidence_threshold: Threshold for the confidence score.\n",
    "            data_format: The input format.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        if not data_format:\n",
    "            if tf.test.is_built_with_cuda():\n",
    "                data_format = 'channels_first'\n",
    "            else:\n",
    "                data_format = 'channels_last'\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.model_size = model_size\n",
    "        self.max_output_size = max_output_size\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.data_format = data_format\n",
    "\n",
    "    def __call__(self, inputs, training):\n",
    "        \"\"\"Add operations to detect boxes for a batch of input images.\n",
    "\n",
    "        Args:\n",
    "            inputs: A Tensor representing a batch of input images.\n",
    "            training: A boolean, whether to use in training or inference mode.\n",
    "\n",
    "        Returns:\n",
    "            A list containing class-to-boxes dictionaries\n",
    "                for each sample in the batch.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('yolo_v3_model'):\n",
    "            if self.data_format == 'channels_first':\n",
    "                inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "            inputs = inputs / 255\n",
    "\n",
    "            route1, route2, inputs = darknet53(inputs, training=training,\n",
    "                                               data_format=self.data_format)\n",
    "\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=512, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect1 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[6:9],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1,\n",
    "                                          data_format=self.data_format)\n",
    "            inputs = batch_norm(inputs, training=training,\n",
    "                                data_format=self.data_format)\n",
    "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "            upsample_size = route2.get_shape().as_list()\n",
    "            inputs = upsample(inputs, out_shape=upsample_size,\n",
    "                              data_format=self.data_format)\n",
    "            axis = 1 if self.data_format == 'channels_first' else 3\n",
    "            inputs = tf.concat([inputs, route2], axis=axis)\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=256, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect2 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[3:6],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1,\n",
    "                                          data_format=self.data_format)\n",
    "            inputs = batch_norm(inputs, training=training,\n",
    "                                data_format=self.data_format)\n",
    "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "            upsample_size = route1.get_shape().as_list()\n",
    "            inputs = upsample(inputs, out_shape=upsample_size,\n",
    "                              data_format=self.data_format)\n",
    "            inputs = tf.concat([inputs, route1], axis=axis)\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=128, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect3 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[0:3],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = tf.concat([detect1, detect2, detect3], axis=1)\n",
    "\n",
    "            inputs = build_boxes(inputs)\n",
    "\n",
    "            boxes_dicts = non_max_suppression(\n",
    "                inputs, n_classes=self.n_classes,\n",
    "                max_output_size=self.max_output_size,\n",
    "                iou_threshold=self.iou_threshold,\n",
    "                confidence_threshold=self.confidence_threshold)\n",
    "\n",
    "            return boxes_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cddc02edf2dbbefe566666d557881faa8b59155e",
    "colab_type": "text",
    "id": "546_fkq_nBik"
   },
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Utility functions\n",
    "Here are some utility functions that will help us load images as NumPy arrays, load class names from the official file and draw the predicted boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "b992d6d649fdcb4c279deee99b2f66362066646c",
    "colab": {},
    "colab_type": "code",
    "id": "GBNQ-nxznBim"
   },
   "outputs": [],
   "source": [
    "def load_images(img_names, model_size):\n",
    "    \"\"\"Loads images in a 4D array.\n",
    "\n",
    "    Args:\n",
    "        img_names: A list of images names.\n",
    "        model_size: The input size of the model.\n",
    "        data_format: A format for the array returned\n",
    "            ('channels_first' or 'channels_last').\n",
    "\n",
    "    Returns:\n",
    "        A 4D NumPy array.\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "\n",
    "    for img_name in img_names:\n",
    "        img = Image.open(img_name)\n",
    "        img = img.resize(size=model_size)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.concatenate(imgs)\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def load_class_names(file_name):\n",
    "    \"\"\"Returns a list of class names read from `file_name`.\"\"\"\n",
    "    with open(file_name, 'r') as f:\n",
    "        class_names = f.read().splitlines()\n",
    "    return class_names\n",
    "\n",
    "image_findingname=[]\n",
    "def draw_boxes(img_names, boxes_dicts, class_names, model_size):\n",
    "    \"\"\"Draws detected boxes.\n",
    "\n",
    "    Args:\n",
    "        img_names: A list of input images names.\n",
    "        boxes_dict: A class-to-boxes dictionary.\n",
    "        class_names: A class names list.\n",
    "        model_size: The input size of the model.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    colors = ((np.array(color_palette(\"hls\", 80)) * 255)).astype(np.uint8)\n",
    "    for num, img_name, boxes_dict in zip(range(len(img_names)), img_names,\n",
    "                                         boxes_dicts):\n",
    "        img = Image.open(img_name)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        font = ImageFont.truetype(font='/content/drive/My Drive/yolo/futur.ttf',\n",
    "                                  size=(img.size[0] + img.size[1]) // 100)\n",
    "        resize_factor = \\\n",
    "            (img.size[0] / model_size[0], img.size[1] / model_size[1])\n",
    "        for cls in range(len(class_names)):\n",
    "            boxes = boxes_dict[cls]\n",
    "            if np.size(boxes) != 0:\n",
    "                color = colors[cls]\n",
    "                # image_findingname.append(cls)\n",
    "                for box in boxes:\n",
    "                    xy, confidence = box[:4], box[4]\n",
    "                    xy = [xy[i] * resize_factor[i % 2] for i in range(4)]\n",
    "                    x0, y0 = xy[0], xy[1]\n",
    "                    thickness = (img.size[0] + img.size[1]) // 200\n",
    "                    for t in np.linspace(0, 1, thickness):\n",
    "                        xy[0], xy[1] = xy[0] + t, xy[1] + t\n",
    "                        xy[2], xy[3] = xy[2] - t, xy[3] - t\n",
    "                        draw.rectangle(xy, outline=tuple(color))\n",
    "                    text = '{} {:.1f}%'.format(class_names[cls],\n",
    "                                               confidence * 100)\n",
    "                    text1='{} '.format(class_names[cls])\n",
    "                    image_findingname.append(text1)\n",
    "                    text_size = draw.textsize(text, font=font)\n",
    "                    draw.rectangle(\n",
    "                        [x0, y0 - text_size[1], x0 + text_size[0], y0],\n",
    "                        fill=tuple(color))\n",
    "                    draw.text((x0, y0 - text_size[1]), text, fill='black',\n",
    "                              font=font)\n",
    "\n",
    "        display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13f8a2f689aff3e5941e94343a2f060f9049917f",
    "colab_type": "text",
    "id": "9qISg99OnBiy"
   },
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. Converting weights to Tensorflow format\n",
    "Now it's time to load the official weights. We are going to iterate through the file and gradually create `tf.assign` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24d4689103695a95a2634c4d688475ba2750fdea",
    "colab": {},
    "colab_type": "code",
    "id": "Q06N9xCznBi0"
   },
   "outputs": [],
   "source": [
    "def load_weights(variables, file_name):\n",
    "    \"\"\"Reshapes and loads official pretrained Yolo weights.\n",
    "\n",
    "    Args:\n",
    "        variables: A list of tf.Variable to be assigned.\n",
    "        file_name: A name of a file containing weights.\n",
    "\n",
    "    Returns:\n",
    "        A list of assign operations.\n",
    "    \"\"\"\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        # Skip first 5 values containing irrelevant info\n",
    "        np.fromfile(f, dtype=np.int32, count=5)\n",
    "        weights = np.fromfile(f, dtype=np.float32)\n",
    "\n",
    "        assign_ops = []\n",
    "        ptr = 0\n",
    "\n",
    "        # Load weights for Darknet part.\n",
    "        # Each convolution layer has batch normalization.\n",
    "        for i in range(52):\n",
    "            conv_var = variables[5 * i]\n",
    "            gamma, beta, mean, variance = variables[5 * i + 1:5 * i + 5]\n",
    "            batch_norm_vars = [beta, gamma, mean, variance]\n",
    "\n",
    "            for var in batch_norm_vars:\n",
    "                shape = var.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(var, var_weights))\n",
    "\n",
    "            shape = conv_var.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                (shape[3], shape[2], shape[0], shape[1]))\n",
    "            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "        # Loading weights for Yolo part.\n",
    "        # 7th, 15th and 23rd convolution layer has biases and no batch norm.\n",
    "        ranges = [range(0, 6), range(6, 13), range(13, 20)]\n",
    "        unnormalized = [6, 13, 20]\n",
    "        for j in range(3):\n",
    "            for i in ranges[j]:\n",
    "                current = 52 * 5 + 5 * i + j * 2\n",
    "                conv_var = variables[current]\n",
    "                gamma, beta, mean, variance =  \\\n",
    "                    variables[current + 1:current + 5]\n",
    "                batch_norm_vars = [beta, gamma, mean, variance]\n",
    "\n",
    "                for var in batch_norm_vars:\n",
    "                    shape = var.shape.as_list()\n",
    "                    num_params = np.prod(shape)\n",
    "                    var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "                    ptr += num_params\n",
    "                    assign_ops.append(tf.assign(var, var_weights))\n",
    "\n",
    "                shape = conv_var.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                    (shape[3], shape[2], shape[0], shape[1]))\n",
    "                var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "            bias = variables[52 * 5 + unnormalized[j] * 5 + j * 2 + 1]\n",
    "            shape = bias.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(bias, var_weights))\n",
    "\n",
    "            conv_var = variables[52 * 5 + unnormalized[j] * 5 + j * 2]\n",
    "            shape = conv_var.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                (shape[3], shape[2], shape[0], shape[1]))\n",
    "            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "    return assign_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4ce260c2e3c4f0aad9e711796955586b5f0e674",
    "colab_type": "text",
    "id": "xFxVrWjenBjG"
   },
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. Running the model\n",
    "Now we can run the model using some sample images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed0422e95da34c20827fe85a31907fe7d8b3a648",
    "colab_type": "text",
    "id": "cIXu0tWfnBjJ"
   },
   "source": [
    "### Sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4db589483f06d03a5053efb80a624b54517925f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "RLmcQ5BbnBjM",
    "outputId": "8a201c9b-f0a8-416d-a732-fb9be8f2b7c3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img_names = ['/content/drive/My Drive/yolo/input003/image1.jpg', '/content/drive/My Drive/yolo/office.jpg']\n",
    "# img_names = ['/content/drive/My Drive/yolo/office.jpg']\n",
    "img_names = ['/content/drive/My Drive/yolo/ImageTraffic.jpg']\n",
    "for img in img_names: display(Image.open(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9NXlYXunBje"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40b4a8d68a58cc2c2fcccf0dc24345f92ae955c2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "W6BZRfc9nBjr",
    "outputId": "3c926e47-b3c8-4384-b364-b212920cc803",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = len(img_names)\n",
    "batch = load_images(img_names, model_size=_MODEL_SIZE)\n",
    "class_names = load_class_names('/content/drive/My Drive/yolo/coco.names')\n",
    "n_classes = len(class_names)\n",
    "max_output_size = 100\n",
    "iou_threshold = 0.5\n",
    "confidence_threshold = 0.1\n",
    "\n",
    "model = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE,\n",
    "                max_output_size=max_output_size,\n",
    "                iou_threshold=iou_threshold,\n",
    "                confidence_threshold=confidence_threshold)\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [batch_size, 416, 416, 3])\n",
    "\n",
    "detections = model(inputs, training=False)\n",
    "\n",
    "model_vars = tf.global_variables(scope='yolo_v3_model')\n",
    "assign_ops = load_weights(model_vars, '/content/drive/My Drive/yolo/yolov3.weights')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(assign_ops)\n",
    "    detection_result = sess.run(detections, feed_dict={inputs: batch})\n",
    "    \n",
    "draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1W_tTdhdxyGq",
    "outputId": "bffc32c3-9005-4185-fc7c-633193503b05"
   },
   "outputs": [],
   "source": [
    "coco_names_list=[0,1,2,3,5,7]\n",
    "detection_result[0][2][1][0:4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d99pzZy9NPK9"
   },
   "outputs": [],
   "source": [
    "object_detect_data=[]\n",
    "tracking_id=''\n",
    "for i in coco_names_list:\n",
    "  result=detection_result[0][i]\n",
    "  for j in result:\n",
    "    object_position_x=j[0:4][0]\n",
    "    object_position_y=j[0:4][1]\n",
    "    object_width=j[0:4][2]\n",
    "    object_height=j[0:4][3]\n",
    "    if i==0:\n",
    "      tracking_id=\"PD\"\n",
    "    elif i==1:\n",
    "      tracking_id=\"BK\"\n",
    "    elif i==2:\n",
    "      tracking_id=\"LV\"\n",
    "    elif i==3:\n",
    "      tracking_id=\"MC\"\n",
    "    elif i==5:\n",
    "      tracking_id=\"BS\"\n",
    "    elif i==7:\n",
    "      tracking_id=\"TR\"\n",
    "    a=tracking_id,object_position_x,object_position_y,object_width,object_height\n",
    "    object_detect_data.append(a)\n",
    "    # print(tracking_id,object_position_x,object_position_y,object_width,object_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_PcsMFkYaKJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datagen=pd.DataFrame(object_detect_data,columns=['object_code','object_position_x', 'object_position_y','object_width','object_height'])\n",
    "datagen.index.name = 'Frame_number'\n",
    "datagen.to_csv(\"/content/drive/My Drive/yolo/input003/object_detectn_boundingboxs_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "fb46fa422366237bac8dfe281950aa44e7a9c798",
    "colab": {},
    "colab_type": "code",
    "id": "pFjEbmPrnBj1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# with open('/content/drive/My Drive/yolo/detections.gif','rb') as f:\n",
    "#     display(Image(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-X12pmqES-39"
   },
   "source": [
    "#Face identificasction with bounding box ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-aA3wlonBkY"
   },
   "outputs": [],
   "source": [
    "img1 = cv2.imread('/content/drive/My Drive/yolo/input003/image1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0wDzKVgnBkj"
   },
   "outputs": [],
   "source": [
    "img1 = cv2.resize(img1, (416, 416)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWi6Qx5UnBkr"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_image(image):\n",
    "#     plt.figure(figsize=(18,15))\n",
    "    #Before showing image, bgr color order transformed to rgb order\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "TUfiq6JsnBld",
    "outputId": "09ee681a-78e2-4843-e207-95a05882e7bf"
   },
   "outputs": [],
   "source": [
    "per = img1[30:400, 150:280]\n",
    "value=show_image(per)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DfSzuW65TOeP"
   },
   "source": [
    "#croped image  save wite output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umwwDyDLnBln"
   },
   "outputs": [],
   "source": [
    "# # import scipy.misc\n",
    "# # scipy.misc.imsave('outfile1.png', per)\n",
    "# import imageio\n",
    "# imageio.imwrite('/content/drive/My Drive/yolo/input003/outfile1.jpg', per)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "object_detect_senddata_tocsv_yolo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
